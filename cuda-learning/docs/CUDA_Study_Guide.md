# CUDAå…±äº«å†…å­˜ä¸æ€§èƒ½ä¼˜åŒ–å­¦ä¹ æŒ‡å—

## ğŸ“š ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç¯å¢ƒè®¾ç½®](#ç¯å¢ƒè®¾ç½®)
3. [å…±äº«å†…å­˜åŸºç¡€](#å…±äº«å†…å­˜åŸºç¡€)
4. [çŸ©é˜µä¹˜æ³•ä¼˜åŒ–æ¼”è¿›](#çŸ©é˜µä¹˜æ³•ä¼˜åŒ–æ¼”è¿›)
5. [æ€§èƒ½åˆ†ææ–¹æ³•](#æ€§èƒ½åˆ†ææ–¹æ³•)
6. [é«˜çº§å…±äº«å†…å­˜æŠ€æœ¯](#é«˜çº§å…±äº«å†…å­˜æŠ€æœ¯)
7. [å®è·µç»ƒä¹ ](#å®è·µç»ƒä¹ )
8. [å¸¸è§é—®é¢˜è§£ç­”](#å¸¸è§é—®é¢˜è§£ç­”)
9. [è¿›é˜¶å­¦ä¹ èµ„æº](#è¿›é˜¶å­¦ä¹ èµ„æº)

---

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®é€šè¿‡4ä¸ªé€’è¿›çš„å®æˆ˜ç¤ºä¾‹ï¼Œå¸®åŠ©æ‚¨æŒæ¡CUDAå…±äº«å†…å­˜çš„ä½¿ç”¨ã€kernelæ€§èƒ½åˆ†ææ–¹æ³•å’ŒçŸ©é˜µä¹˜æ³•ä¼˜åŒ–æŠ€æœ¯ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£CUDAå†…å­˜å±‚æ¬¡ç»“æ„
- æŒæ¡å…±äº«å†…å­˜çš„æœ‰æ•ˆä½¿ç”¨æ–¹æ³•
- å­¦ä¼šåˆ†æå’Œä¼˜åŒ–CUDA kernelæ€§èƒ½
- å®ç°é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•ç®—æ³•
- æŒæ¡æ€§èƒ½åˆ†æå·¥å…·çš„ä½¿ç”¨

### ğŸ“‚ é¡¹ç›®ç»“æ„

```
cuda-learning/
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ 01_shared_memory_basics.cu      # å…±äº«å†…å­˜åŸºç¡€
â”‚   â”œâ”€â”€ 02_matrix_multiply_evolution.cu # çŸ©é˜µä¹˜æ³•æ¼”è¿›
â”‚   â”œâ”€â”€ 03_performance_analysis.cu      # æ€§èƒ½åˆ†ææ–¹æ³•
â”‚   â””â”€â”€ 04_advanced_shared_memory.cu    # é«˜çº§å…±äº«å†…å­˜æŠ€æœ¯
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ compile_and_run.sh             # ç¼–è¯‘è¿è¡Œè„šæœ¬
â”œâ”€â”€ build/                             # æ„å»ºè¾“å‡ºç›®å½•
â”œâ”€â”€ docs/                              # æ–‡æ¡£
â”œâ”€â”€ Makefile                           # æ„å»ºé…ç½®
â””â”€â”€ README.md                          # é¡¹ç›®è¯´æ˜
```

---

## ç¯å¢ƒè®¾ç½®

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Ubuntu 18.04+ / CentOS 7+ / Windows 10+
- **GPU**: NVIDIA GPU (è®¡ç®—èƒ½åŠ› 3.0+)
- **CUDAå·¥å…·åŒ…**: 10.0+
- **ç¼–è¯‘å™¨**: GCC 7+ / MSVC 2017+

### ğŸ”§ å®‰è£…CUDAå·¥å…·åŒ…

#### Ubuntu/Debian:
```bash
# æ·»åŠ NVIDIAåŒ…ä»“åº“
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"

# å®‰è£…CUDA
sudo apt update
sudo apt install cuda-toolkit-11-8
```

#### æˆ–è€…ä½¿ç”¨åŒ…ç®¡ç†å™¨å¿«é€Ÿå®‰è£…ï¼š
```bash
sudo apt install nvidia-cuda-toolkit
```

### âœ… éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥CUDAç¼–è¯‘å™¨
nvcc --version

# æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
nvidia-smi -L
```

### ğŸš€ ç¼–è¯‘å’Œè¿è¡Œ

#### æ–¹æ³•1: ä½¿ç”¨Makefile
```bash
# ç¼–è¯‘æ‰€æœ‰ç¤ºä¾‹
make

# è¿è¡Œç‰¹å®šç¤ºä¾‹
make run-example1

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
make examples

# æ€§èƒ½åˆ†æ
make profile
```

#### æ–¹æ³•2: ä½¿ç”¨è„šæœ¬
```bash
# ä½¿è„šæœ¬å¯æ‰§è¡Œ
chmod +x scripts/compile_and_run.sh

# ç¼–è¯‘å¹¶è¿è¡Œç¤ºä¾‹1
./scripts/compile_and_run.sh 1

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
./scripts/compile_and_run.sh -a

# æ€§èƒ½åˆ†æ
./scripts/compile_and_run.sh -p 3
```

#### æ–¹æ³•3: æ‰‹åŠ¨ç¼–è¯‘
```bash
mkdir -p build
nvcc -O3 -arch=sm_50 src/01_shared_memory_basics.cu -o build/example1
./build/example1
```

---

## å…±äº«å†…å­˜åŸºç¡€

### ğŸ§  ç†è®ºåŸºç¡€

#### CUDAå†…å­˜å±‚æ¬¡ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å…¨å±€å†…å­˜ (Global Memory)                 â”‚ â† å¤§å®¹é‡ï¼Œé«˜å»¶è¿Ÿ
â”‚ â€¢ å®¹é‡: GBçº§                             â”‚
â”‚ â€¢ å»¶è¿Ÿ: 400-800 cycles                   â”‚
â”‚ â€¢ å¸¦å®½: ~1TB/s                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2ç¼“å­˜ (L2 Cache)                       â”‚
â”‚ â€¢ å®¹é‡: MBçº§                             â”‚
â”‚ â€¢ å»¶è¿Ÿ: 200-300 cycles                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1ç¼“å­˜/å…±äº«å†…å­˜ (L1/Shared Memory)        â”‚ â† å°å®¹é‡ï¼Œä½å»¶è¿Ÿ
â”‚ â€¢ å®¹é‡: 48-164KB                         â”‚
â”‚ â€¢ å»¶è¿Ÿ: 20-30 cycles                     â”‚
â”‚ â€¢ å¸¦å®½: >8TB/s                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¯„å­˜å™¨ (Registers)                       â”‚ â† æœ€å¿«
â”‚ â€¢ å»¶è¿Ÿ: 1 cycle                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å…±äº«å†…å­˜ç‰¹ç‚¹

1. **å¿«é€Ÿè®¿é—®**: å»¶è¿Ÿæ¯”å…¨å±€å†…å­˜ä½10-100å€
2. **å—å†…å…±äº«**: åŒä¸€çº¿ç¨‹å—å†…çš„æ‰€æœ‰çº¿ç¨‹å¯ä»¥è®¿é—®
3. **ç¨‹åºå‘˜ç®¡ç†**: éœ€è¦æ˜¾å¼å£°æ˜å’Œç®¡ç†
4. **å®¹é‡æœ‰é™**: æ¯ä¸ªSMåªæœ‰48-164KB
5. **Bankç»“æ„**: åˆ†ä¸º32ä¸ªbankï¼Œé¿å…å†²çªå¾ˆé‡è¦

### ğŸ’¡ ç¤ºä¾‹1: å…±äº«å†…å­˜åŸºç¡€ (`01_shared_memory_basics.cu`)

#### ä¸»è¦å†…å®¹ï¼š
- å…¨å±€å†…å­˜ vs å…±äº«å†…å­˜æ€§èƒ½å¯¹æ¯”
- å‘é‡å½’çº¦çš„ä¸åŒå®ç°
- Bankå†²çªæ¼”ç¤ºå’Œé¿å…æ–¹æ³•

#### å…³é”®ä»£ç è§£æï¼š

```cuda
// ä½¿ç”¨å…±äº«å†…å­˜çš„å½’çº¦
__global__ void vector_reduce_shared(float* input, float* output, int n) {
    extern __shared__ float sdata[];  // åŠ¨æ€åˆ†é…å…±äº«å†…å­˜
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 1. åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜
    sdata[tid] = (idx < n) ? input[idx] : 0.0f;
    __syncthreads();  // åŒæ­¥ç­‰å¾…æ‰€æœ‰çº¿ç¨‹åŠ è½½å®Œæˆ
    
    // 2. åœ¨å…±äº«å†…å­˜ä¸­è¿›è¡Œå½’çº¦
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();  // æ¯è½®å½’çº¦ååŒæ­¥
    }
    
    // 3. ç¬¬ä¸€ä¸ªçº¿ç¨‹å†™å›ç»“æœ
    if (tid == 0) {
        atomicAdd(output, sdata[0]);
    }
}
```

#### æ€§èƒ½æå‡åˆ†æï¼š
- **å‡å°‘å…¨å±€å†…å­˜è®¿é—®**: æ¯ä¸ªæ•°æ®åªä»å…¨å±€å†…å­˜è¯»å–ä¸€æ¬¡
- **é«˜é€Ÿå…±äº«å†…å­˜è®¡ç®—**: å½’çº¦æ“ä½œåœ¨å…±äº«å†…å­˜ä¸­è¿›è¡Œ
- **åˆå¹¶å†…å­˜è®¿é—®**: è¿ç»­çš„çº¿ç¨‹è®¿é—®è¿ç»­çš„å†…å­˜åœ°å€

---

## çŸ©é˜µä¹˜æ³•ä¼˜åŒ–æ¼”è¿›

### ğŸ¯ ä¼˜åŒ–ç›®æ ‡

å°†åŸºç¡€çš„çŸ©é˜µä¹˜æ³•ä»**æœ´ç´ ç‰ˆæœ¬**é€æ­¥ä¼˜åŒ–åˆ°**é«˜æ€§èƒ½ç‰ˆæœ¬**ï¼Œå±•ç¤ºæ¯ä¸ªä¼˜åŒ–æ­¥éª¤çš„æ•ˆæœã€‚

### ğŸ“Š ç¤ºä¾‹2: çŸ©é˜µä¹˜æ³•æ¼”è¿› (`02_matrix_multiply_evolution.cu`)

#### ç‰ˆæœ¬1: æœ´ç´ å®ç°
```cuda
__global__ void matrix_mul_naive(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];  // å¤§é‡å…¨å±€å†…å­˜è®¿é—®
        }
        C[row * N + col] = sum;
    }
}
```

**é—®é¢˜**: 
- æ¯ä¸ªçº¿ç¨‹é‡å¤è¯»å–ç›¸åŒçš„Aå’ŒBå…ƒç´ 
- å…¨å±€å†…å­˜è®¿é—®å»¶è¿Ÿé«˜
- å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ä½

#### ç‰ˆæœ¬2: åŸºç¡€Tiling (å…±äº«å†…å­˜)
```cuda
__global__ void matrix_mul_shared_basic(float* A, float* B, float* C, int N) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    // åä½œåŠ è½½tileåˆ°å…±äº«å†…å­˜
    int tx = threadIdx.x, ty = threadIdx.y;
    int row = blockIdx.y * TILE_SIZE + ty;
    int col = blockIdx.x * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    // å¾ªç¯å¤„ç†æ‰€æœ‰tile
    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {
        // åŠ è½½å½“å‰tile
        if (row < N && t * TILE_SIZE + tx < N) {
            As[ty][tx] = A[row * N + t * TILE_SIZE + tx];
        } else {
            As[ty][tx] = 0.0f;
        }
        
        if (col < N && t * TILE_SIZE + ty < N) {
            Bs[ty][tx] = B[(t * TILE_SIZE + ty) * N + col];
        } else {
            Bs[ty][tx] = 0.0f;
        }
        
        __syncthreads();
        
        // è®¡ç®—å½“å‰tileçš„è´¡çŒ®
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[ty][k] * Bs[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}
```

**æ”¹è¿›**:
- æ•°æ®é‡ç”¨: æ¯ä¸ªtileè¢«å—å†…æ‰€æœ‰çº¿ç¨‹å…±äº«
- å‡å°‘å…¨å±€å†…å­˜è®¿é—®: æ•°æ®åªåŠ è½½ä¸€æ¬¡åˆ°å…±äº«å†…å­˜
- æå‡å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

#### ç‰ˆæœ¬3: é¿å…Bankå†²çª
```cuda
__global__ void matrix_mul_shared_optimized(float* A, float* B, float* C, int N) {
    // ä½¿ç”¨å¡«å……é¿å…bankå†²çª
    __shared__ float As[TILE_SIZE][TILE_SIZE + 1];  // +1æ˜¯å…³é”®!
    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];
    
    // ... ç›¸åŒçš„è®¡ç®—é€»è¾‘ä½†é¿å…äº†bankå†²çª
}
```

**ä¼˜åŒ–ç‚¹**:
- **é¿å…Bankå†²çª**: `+1`å¡«å……ç¡®ä¿åˆ—è®¿é—®ä¸ä¼šäº§ç”Ÿbankå†²çª
- **æå‡å†…å­˜è®¿é—®æ•ˆç‡**: å…±äº«å†…å­˜è®¿é—®æ›´åŠ é«˜æ•ˆ

#### æ€§èƒ½å¯¹æ¯”åˆ†æ

| ç‰ˆæœ¬ | æ—¶é—´(ms) | ç›¸æ¯”CPU | ç›¸æ¯”æœ´ç´ GPU | ä¸»è¦ä¼˜åŒ– |
|------|----------|---------|-------------|----------|
| CPU | 8000 | 1.0x | - | ä¸²è¡Œè®¡ç®— |
| GPUæœ´ç´  | 400 | 20x | 1.0x | å¹¶è¡Œè®¡ç®— |
| GPUå…±äº«å†…å­˜(åŸºç¡€) | 80 | 100x | 5x | æ•°æ®é‡ç”¨ |
| GPUå…±äº«å†…å­˜(ä¼˜åŒ–) | 60 | 133x | 6.7x | é¿å…bankå†²çª |
| cuBLAS | 15 | 533x | 26.7x | é«˜åº¦ä¼˜åŒ–åº“ |

---

## æ€§èƒ½åˆ†ææ–¹æ³•

### ğŸ” ç¤ºä¾‹3: æ€§èƒ½åˆ†ææ–¹æ³• (`03_performance_analysis.cu`)

#### åˆ†æç»´åº¦

1. **å†…å­˜è®¿é—®æ¨¡å¼**
   - åˆå¹¶è®¿é—® vs è·¨æ­¥è®¿é—® vs éšæœºè®¿é—®
   - ç¼“å­˜å‘½ä¸­ç‡åˆ†æ

2. **è®¡ç®— vs å†…å­˜å¯†é›†å‹**
   - ç®—æœ¯å¼ºåº¦ (Arithmetic Intensity)
   - è®¡ç®—ååé‡ vs å†…å­˜å¸¦å®½

3. **åˆ†æ”¯åˆ†åŒ–**
   - Warpå†…çº¿ç¨‹æ‰§è¡Œä¸€è‡´æ€§
   - åˆ†æ”¯æ•ˆç‡åˆ†æ

4. **å ç”¨ç‡åˆ†æ**
   - SMåˆ©ç”¨ç‡
   - èµ„æºä½¿ç”¨æƒ…å†µ

#### å…³é”®æ€§èƒ½æŒ‡æ ‡

```cuda
// å†…å­˜å¸¦å®½è®¡ç®—
float measure_bandwidth(float* d_input, float* d_output, int n, int iterations) {
    // è®¡ç®—æœ‰æ•ˆå¸¦å®½
    float bytes_transferred = 2.0f * n * sizeof(float) * iterations;  // è¯»+å†™
    float bandwidth = bytes_transferred / (time_ms / 1000.0f) / (1024*1024*1024);
    return bandwidth;
}

// å ç”¨ç‡åˆ†æ
void analyze_occupancy(const void* kernel, int block_size) {
    int max_active_blocks;
    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks, kernel, block_size, 0);
    
    float occupancy = (max_active_blocks * block_size / (float)prop.maxThreadsPerMultiProcessor) * 100;
    printf("ç†è®ºå ç”¨ç‡: %.1f%%\n", occupancy);
}
```

### ğŸ› ï¸ æ€§èƒ½åˆ†æå·¥å…·é“¾

#### 1. CUDA Events (åŸºç¡€è®¡æ—¶)
```cuda
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start);
kernel<<<grid, block>>>(args);
cudaEventRecord(stop);
cudaEventSynchronize(stop);

float time_ms;
cudaEventElapsedTime(&time_ms, start, stop);
```

#### 2. nvprof (å·²å¼ƒç”¨ï¼Œä½†ä»æœ‰ç”¨)
```bash
# åŸºç¡€æ€§èƒ½åˆ†æ
nvprof ./your_program

# ç‰¹å®šæŒ‡æ ‡åˆ†æ
nvprof --metrics achieved_occupancy,gld_efficiency,gst_efficiency ./your_program

# ç”Ÿæˆåˆ†ææŠ¥å‘Š
nvprof --analysis-metrics -o profile.nvvp ./your_program
```

#### 3. Nsight Compute (æ¨è)
```bash
# å®Œæ•´åˆ†æ
ncu --set full ./your_program

# ç‰¹å®šæŒ‡æ ‡
ncu --metrics sm__cycles_elapsed.avg,dram__bytes_read.sum ./your_program

# ç”ŸæˆæŠ¥å‘Š
ncu --set basic -o profile ./your_program
```

#### 4. Nsight Systems (ç³»ç»Ÿçº§åˆ†æ)
```bash
# æ—¶é—´çº¿åˆ†æ
nsys profile -t cuda ./your_program

# ç”ŸæˆæŠ¥å‘Š
nsys profile -o profile ./your_program
```

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æµç¨‹

1. **åŸºå‡†æµ‹è¯•**: å»ºç«‹æ€§èƒ½åŸºçº¿
2. **ç“¶é¢ˆè¯†åˆ«**: æ‰¾å‡ºé™åˆ¶æ€§èƒ½çš„å› ç´ 
3. **æœ‰é’ˆå¯¹æ€§ä¼˜åŒ–**: è§£å†³ä¸»è¦ç“¶é¢ˆ
4. **éªŒè¯æ”¹è¿›**: æµ‹é‡ä¼˜åŒ–æ•ˆæœ
5. **è¿­ä»£ä¼˜åŒ–**: é‡å¤ä¸Šè¿°è¿‡ç¨‹

---

## é«˜çº§å…±äº«å†…å­˜æŠ€æœ¯

### ğŸš€ ç¤ºä¾‹4: é«˜çº§å…±äº«å†…å­˜æŠ€æœ¯ (`04_advanced_shared_memory.cu`)

#### Bankå†²çªæ·±å…¥ç†è§£

å…±äº«å†…å­˜åˆ†ä¸º32ä¸ªbankï¼Œæ¯ä¸ªbankæ¯ä¸ªæ—¶é’Ÿå‘¨æœŸå¯ä»¥æœåŠ¡ä¸€ä¸ªè¯·æ±‚ï¼š

```
Bank 0: åœ°å€ 0, 32, 64, 96, ...
Bank 1: åœ°å€ 1, 33, 65, 97, ...
...
Bank 31: åœ°å€ 31, 63, 95, 127, ...
```

#### é¿å…Bankå†²çªçš„æŠ€æœ¯

1. **å¡«å……æŠ€æœ¯**
```cuda
// äº§ç”Ÿå†²çªçš„å£°æ˜
__shared__ float data[32][32];

// é¿å…å†²çªçš„å£°æ˜  
__shared__ float data[32][33];  // +1å¡«å……é¿å…åˆ—è®¿é—®å†²çª
```

2. **è®¿é—®æ¨¡å¼ä¼˜åŒ–**
```cuda
// å†²çªçš„è®¿é—®æ¨¡å¼
data[threadIdx.x * 2]  // 2è·¯bankå†²çª

// ä¼˜åŒ–çš„è®¿é—®æ¨¡å¼
data[threadIdx.x]      // æ— å†²çª
```

#### åŒç¼“å†²æŠ€æœ¯

```cuda
__global__ void double_buffering_example(float* input, float* output, int n) {
    extern __shared__ float sdata[];
    
    // åŒç¼“å†²åŒº
    float* buffer_a = sdata;
    float* buffer_b = &sdata[tile_size];
    
    // é¢„åŠ è½½ç¬¬ä¸€ä¸ªtile
    buffer_a[tid] = input[tid];
    __syncthreads();
    
    for (int tile = 0; tile < num_tiles - 1; tile++) {
        // å½“å‰tileè®¡ç®— (ä½¿ç”¨buffer_a)
        float result = compute(buffer_a[tid]);
        
        // åŒæ—¶åŠ è½½ä¸‹ä¸€ä¸ªtile (åˆ°buffer_b)
        buffer_b[tid] = input[next_tile_offset];
        __syncthreads();
        
        // å†™å›ç»“æœ
        output[current_offset] = result;
        
        // äº¤æ¢ç¼“å†²åŒº
        swap(buffer_a, buffer_b);
        __syncthreads();
    }
}
```

#### Warpçº§åˆ«åŸè¯­

```cuda
// ä½¿ç”¨shuffleæŒ‡ä»¤è¿›è¡Œwarpå†…å½’çº¦
__global__ void warp_reduce(float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    float val = (idx < n) ? input[idx] : 0.0f;
    
    // Warpå†…å½’çº¦ï¼Œæ— éœ€å…±äº«å†…å­˜
    for (int offset = 16; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    
    // åªæœ‰lane 0å†™å…¥ç»“æœ
    if (threadIdx.x % 32 == 0) {
        atomicAdd(output, val);
    }
}
```

#### çŸ©é˜µè½¬ç½®ä¼˜åŒ–

```cuda
__global__ void matrix_transpose_optimized(float* input, float* output, int width, int height) {
    __shared__ float tile[TILE_SIZE][TILE_SIZE + 1];  // é¿å…bankå†²çª
    
    // è®¡ç®—è¾“å…¥å’Œè¾“å‡ºåæ ‡
    int x_in = blockIdx.x * TILE_SIZE + threadIdx.x;
    int y_in = blockIdx.y * TILE_SIZE + threadIdx.y;
    int x_out = blockIdx.y * TILE_SIZE + threadIdx.x;
    int y_out = blockIdx.x * TILE_SIZE + threadIdx.y;
    
    // åˆå¹¶åŠ è½½åˆ°å…±äº«å†…å­˜
    if (x_in < width && y_in < height) {
        tile[threadIdx.y][threadIdx.x] = input[y_in * width + x_in];
    }
    __syncthreads();
    
    // è½¬ç½®ååˆå¹¶å†™å…¥
    if (x_out < height && y_out < width) {
        output[y_out * height + x_out] = tile[threadIdx.x][threadIdx.y];
    }
}
```

---

## å®è·µç»ƒä¹ 

### ğŸ¯ ç»ƒä¹ 1: å…±äº«å†…å­˜å½’çº¦ä¼˜åŒ–

**ä»»åŠ¡**: å®ç°ä¸€ä¸ªé«˜æ•ˆçš„æ•°ç»„æ±‚å’Œkernel

**è¦æ±‚**:
1. ä½¿ç”¨å…±äº«å†…å­˜
2. é¿å…bankå†²çª
3. å¤„ç†ä»»æ„å¤§å°çš„æ•°ç»„
4. å¯¹æ¯”ä¸åŒå®ç°çš„æ€§èƒ½

**æç¤º**:
```cuda
// æ¨¡æ¿ä»£ç 
__global__ void array_sum(float* input, float* output, int n) {
    extern __shared__ float sdata[];
    
    // TODO: å®ç°ä½ çš„å½’çº¦ç®—æ³•
    // è€ƒè™‘ï¼š
    // 1. å¦‚ä½•å¤„ç†æ•°ç»„å¤§å°ä¸æ˜¯å—å¤§å°å€æ•°çš„æƒ…å†µï¼Ÿ
    // 2. å¦‚ä½•é¿å…bankå†²çªï¼Ÿ
    // 3. å¦‚ä½•å‡å°‘warpåˆ†åŒ–ï¼Ÿ
}
```

### ğŸ¯ ç»ƒä¹ 2: ä¼˜åŒ–å·ç§¯æ“ä½œ

**ä»»åŠ¡**: å®ç°2Då·ç§¯çš„å…±äº«å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬

**è¦æ±‚**:
1. æ”¯æŒä»»æ„å¤§å°çš„è¾“å…¥å’Œå·ç§¯æ ¸
2. ä½¿ç”¨å…±äº«å†…å­˜å‡å°‘å…¨å±€å†…å­˜è®¿é—®
3. å¤„ç†è¾¹ç•Œæ¡ä»¶
4. æµ‹é‡å†…å­˜å¸¦å®½åˆ©ç”¨ç‡

### ğŸ¯ ç»ƒä¹ 3: åˆ†æå’Œä¼˜åŒ–ç°æœ‰kernel

**ä»»åŠ¡**: é€‰æ‹©ä¸€ä¸ªç°æœ‰çš„CUDA kernelè¿›è¡Œæ€§èƒ½åˆ†æå’Œä¼˜åŒ–

**æ­¥éª¤**:
1. ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·è¯†åˆ«ç“¶é¢ˆ
2. åˆ¶å®šä¼˜åŒ–è®¡åˆ’
3. å®ç°ä¼˜åŒ–æ–¹æ¡ˆ
4. éªŒè¯æ€§èƒ½æå‡
5. æ’°å†™ä¼˜åŒ–æŠ¥å‘Š

---

## å¸¸è§é—®é¢˜è§£ç­”

### â“ Q1: ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨å…±äº«å†…å­˜ï¼Ÿ

**A**: å½“æ»¡è¶³ä»¥ä¸‹æ¡ä»¶æ—¶è€ƒè™‘ä½¿ç”¨å…±äº«å†…å­˜ï¼š
- åŒä¸€çº¿ç¨‹å—å†…çš„çº¿ç¨‹éœ€è¦å¤šæ¬¡è®¿é—®ç›¸åŒæ•°æ®
- æ•°æ®è®¿é—®æ¨¡å¼å¯ä»¥é¢„æµ‹
- å…¨å±€å†…å­˜è®¿é—®æ˜¯æ€§èƒ½ç“¶é¢ˆ
- ç®—æ³•å¯ä»¥åˆ©ç”¨æ•°æ®çš„å±€éƒ¨æ€§

### â“ Q2: å¦‚ä½•ç¡®å®šæœ€ä¼˜çš„tileå¤§å°ï¼Ÿ

**A**: è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- **å…±äº«å†…å­˜å®¹é‡**: ä¸èƒ½è¶…è¿‡æ¯ä¸ªSMçš„å…±äº«å†…å­˜é™åˆ¶
- **å¯„å­˜å™¨ä½¿ç”¨**: å½±å“å ç”¨ç‡
- **æ•°æ®é‡ç”¨ç¨‹åº¦**: è¾ƒå¤§çš„tileé€šå¸¸æœ‰æ›´å¥½çš„é‡ç”¨
- **è¾¹ç•Œå¤„ç†**: è¾ƒå°çš„tileè¾¹ç•Œå¼€é”€ç›¸å¯¹è¾ƒå¤§

**ç»éªŒå€¼**: 16x16, 32x32é€šå¸¸æ˜¯å¥½çš„èµ·ç‚¹

### â“ Q3: Bankå†²çªçš„å½±å“æœ‰å¤šå¤§ï¼Ÿ

**A**: 
- **2è·¯å†²çª**: æ€§èƒ½ä¸‹é™çº¦50%
- **4è·¯å†²çª**: æ€§èƒ½ä¸‹é™çº¦75%
- **32è·¯å†²çª**: æ€§èƒ½ä¸‹é™çº¦97%

é¿å…bankå†²çªé€šå¸¸æ˜¯æ€§ä»·æ¯”å¾ˆé«˜çš„ä¼˜åŒ–ã€‚

### â“ Q4: ä¸ºä»€ä¹ˆæˆ‘çš„kernelå ç”¨ç‡å¾ˆä½ï¼Ÿ

**A**: å¯èƒ½çš„åŸå› ï¼š
- **å¯„å­˜å™¨ä½¿ç”¨è¿‡å¤š**: å‡å°‘å±€éƒ¨å˜é‡ï¼Œä½¿ç”¨å¸¸é‡å†…å­˜
- **å…±äº«å†…å­˜ä½¿ç”¨è¿‡å¤š**: å‡å°‘å…±äº«å†…å­˜åˆ†é…
- **çº¿ç¨‹å—å¤ªå¤§**: å°è¯•è¾ƒå°çš„å—å¤§å°
- **æ¡ä»¶åˆ†æ”¯å¤ªå¤š**: å‡å°‘åˆ†æ”¯åˆ†åŒ–

### â“ Q5: å¦‚ä½•å¤„ç†ä¸è§„åˆ™çš„æ•°æ®è®¿é—®æ¨¡å¼ï¼Ÿ

**A**: ç­–ç•¥ï¼š
- **æ•°æ®é‡æ’**: é¢„å¤„ç†æ•°æ®ä½¿å…¶æ›´è§„åˆ™
- **é—´æ¥è®¿é—®**: ä½¿ç”¨ç´¢å¼•æ•°ç»„
- **åˆ†å—å¤„ç†**: å°†ä¸è§„åˆ™é—®é¢˜åˆ†è§£ä¸ºè§„åˆ™å­é—®é¢˜
- **çº¹ç†å†…å­˜**: åˆ©ç”¨ç¡¬ä»¶ç¼“å­˜

---

## è¿›é˜¶å­¦ä¹ èµ„æº

### ğŸ“š æ¨èä¹¦ç±

1. **ã€ŠProfessional CUDA C Programmingã€‹** - John Cheng
   - å…¨é¢çš„CUDAç¼–ç¨‹æŒ‡å—
   - åŒ…å«å¤§é‡å®ç”¨ç¤ºä¾‹

2. **ã€ŠCUDA by Exampleã€‹** - Jason Sanders
   - é€‚åˆåˆå­¦è€…çš„å…¥é—¨ä¹¦ç±
   - é€šè¿‡ç¤ºä¾‹å­¦ä¹ CUDAæ¦‚å¿µ

3. **ã€ŠProgramming Massively Parallel Processorsã€‹** - David Kirk
   - å¹¶è¡Œç¼–ç¨‹çš„ç†è®ºåŸºç¡€
   - GPUæ¶æ„æ·±å…¥è®²è§£

### ğŸŒ åœ¨çº¿èµ„æº

1. **NVIDIA Developer Documentation**
   - https://docs.nvidia.com/cuda/
   - å®˜æ–¹æ–‡æ¡£å’Œç¼–ç¨‹æŒ‡å—

2. **CUDA Zone**
   - https://developer.nvidia.com/cuda-zone
   - å·¥å…·ã€åº“å’Œç¤ºä¾‹ä»£ç 

3. **GTC Talks**
   - https://www.nvidia.com/gtc/
   - GPUæŠ€æœ¯å¤§ä¼šæ¼”è®²è§†é¢‘

### ğŸ› ï¸ å®ç”¨å·¥å…·

1. **Nsight Compute**
   - kernelçº§åˆ«æ€§èƒ½åˆ†æ
   - è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

2. **Nsight Systems**
   - ç³»ç»Ÿçº§åˆ«æ€§èƒ½åˆ†æ
   - æ—¶é—´çº¿å¯è§†åŒ–

3. **CUDA-MEMCHECK**
   - å†…å­˜é”™è¯¯æ£€æµ‹
   - è°ƒè¯•å·¥å…·

### ğŸ“– è¿›é˜¶ä¸»é¢˜

1. **Multi-GPUç¼–ç¨‹**
   - è·¨GPUé€šä¿¡
   - è´Ÿè½½å‡è¡¡ç­–ç•¥

2. **CUDA Streams**
   - å¼‚æ­¥æ‰§è¡Œ
   - å†…å­˜ä¼ è¾“ä¼˜åŒ–

3. **Dynamic Parallelism**
   - è®¾å¤‡ç«¯kernelå¯åŠ¨
   - ä¸è§„åˆ™å¹¶è¡Œæ¨¡å¼

4. **GPUæ¶æ„æ·±å…¥**
   - Turing, Ampereæ¶æ„ç‰¹æ€§
   - Tensor Coresç¼–ç¨‹

5. **CUDAåº“é›†æˆ**
   - cuBLAS, cuDNN, cuFFT
   - Thruståº“ä½¿ç”¨

---

## ğŸ’¡ æ€»ç»“

é€šè¿‡æœ¬é¡¹ç›®çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥æŒæ¡äº†ï¼š

âœ… **CUDAå†…å­˜å±‚æ¬¡ç»“æ„**å’Œå…±äº«å†…å­˜çš„ä½œç”¨  
âœ… **çŸ©é˜µä¹˜æ³•ä¼˜åŒ–**çš„å®Œæ•´è¿‡ç¨‹  
âœ… **æ€§èƒ½åˆ†æå·¥å…·**çš„ä½¿ç”¨æ–¹æ³•  
âœ… **å…±äº«å†…å­˜ä¼˜åŒ–æŠ€æœ¯**çš„å®é™…åº”ç”¨  
âœ… **Bankå†²çªé¿å…**å’Œ**åŒç¼“å†²æŠ€æœ¯**  

### ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

1. **æ·±å…¥ç ”ç©¶CUDAåº“**: å­¦ä¹ cuBLASã€cuDNNç­‰é«˜æ€§èƒ½åº“çš„ä½¿ç”¨
2. **æ¢ç´¢æ–°æ¶æ„ç‰¹æ€§**: äº†è§£Tensor Coresã€Multi-Instance GPUç­‰æ–°æŠ€æœ¯
3. **å®é™…é¡¹ç›®åº”ç”¨**: å°†å­¦åˆ°çš„æŠ€æœ¯åº”ç”¨åˆ°å®é™…çš„è®¡ç®—å¯†é›†å‹é¡¹ç›®ä¸­
4. **æŒç»­æ€§èƒ½è°ƒä¼˜**: å…»æˆæ€§èƒ½åˆ†æå’Œä¼˜åŒ–çš„ä¹ æƒ¯

è®°ä½ï¼š**æ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ç†è®ºä¸å®è·µç›¸ç»“åˆï¼**

---

**ğŸ“§ å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿é€šè¿‡GitHub Issuesæˆ–é‚®ä»¶äº¤æµï¼**